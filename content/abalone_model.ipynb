{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "abalone_model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "7e86631c365131cef62360cd2a60920f2088a85e3b5b379ce36677a33b636180"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "def main_execute(epoch_count = 10, mb_size = 2, report = 2, train_ratio = 0.8):\r\n",
        "    load_dataset()\r\n",
        "    weight_initial, bias_initial = init_param()\r\n",
        "    losses_mean_row, accs_mean_row, final_acc = train_and_test(epoch_count,\r\n",
        "                                                               mb_size, \r\n",
        "                                                               report, \r\n",
        "                                                               train_ratio)\r\n",
        "\r\n",
        "    return weight_initial, bias_initial, losses_mean_row, accs_mean_row, final_acc"
      ],
      "outputs": [],
      "metadata": {
        "id": "ABmq05zMcwPc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "import numpy as pd"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "def load_dataset():\r\n",
        "    with open('DATA\\abalone.csv') as csvfile:\r\n",
        "        csvreader = csv.reader(csvfile)\r\n",
        "        next(csvreader)\r\n",
        "        rows = []\r\n",
        "        for row in csvreader:\r\n",
        "            rows.append(row)\r\n",
        "\r\n",
        "    global data, input_cnt, output_cnt \r\n",
        "    \r\n",
        "    input_cnt, output_cnt = 10, 1\r\n",
        "    data = np.zeros([len(rows), input_cnt + output_cnt])\r\n",
        "\r\n",
        "    for n, row in enumerate(rows):\r\n",
        "        if row[0] == 'M' : data[n, 0] = 1\r\n",
        "        if row[0] == 'F' : data[n, 1] = 1\r\n",
        "        if row[0] == 'I' : data[n, 2] = 1\r\n",
        "        data[n, 3 : ]= row[1:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "UUJg2KOQfdQ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "%run MathUtils.ipynb"
      ],
      "outputs": [],
      "metadata": {
        "id": "4Syqg_8UiDQ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "def init_param():\r\n",
        "    global weight, bias \r\n",
        "\r\n",
        "    weight_initial = []\r\n",
        "    bias_initial   = []\r\n",
        "    weight = np.random.normal(RND_MEAN, RND_STD, size = [input_cnt, output_cnt])\r\n",
        "    bias   = np.zeros([output_cnt])\r\n",
        "    print(\"Initial Weight Value : \\n{}\".format(weight))\r\n",
        "    print(\"Initial Bias Value : \\n{}\".format(bias))\r\n",
        "    weight_initial.append(weight)\r\n",
        "    bias_initial.append(bias)\r\n",
        "\r\n",
        "    return weight_initial, bias_initial"
      ],
      "outputs": [],
      "metadata": {
        "id": "sbIrPR0MiDeD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "def arrange_data(mb_size, train_ratio):\r\n",
        "    \r\n",
        "    global shuffle_map, test_begin_index\r\n",
        "\r\n",
        "    shuffle_map = np.arange(data.shape[0])\r\n",
        "    np.random.shuffle(shuffle_map)\r\n",
        "\r\n",
        "    mini_batch_step_count = int(data.shape[0] * train_ratio) // mb_size\r\n",
        "    test_begin_index = mini_batch_step_count * mb_size\r\n",
        "    return mini_batch_step_count"
      ],
      "outputs": [],
      "metadata": {
        "id": "CK0cGtBwoCzQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "def get_test_data():\r\n",
        "    test_data = data[shuffle_map[test_begin_index:]]\r\n",
        "    return test_data[ : , : -output_cnt], test_data[ : , -output_cnt : ]"
      ],
      "outputs": [],
      "metadata": {
        "id": "msTAQ-XIp-d7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "def get_train_data(mb_size, n):\r\n",
        "    if n == 0:\r\n",
        "        np.random.shuffle(shuffle_map[:test_begin_index])\r\n",
        "\r\n",
        "    train_data = data[shuffle_map[mb_size * n : mb_size * (n+1) ]]\r\n",
        "\r\n",
        "    return train_data[ : , : -output_cnt], train_data[ : , -output_cnt : ]"
      ],
      "outputs": [],
      "metadata": {
        "id": "GJujpEJbrQQP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "def train_and_test(epoch_count, mb_size, report, train_ratio):\r\n",
        "    mini_batch_step_count = arrange_data(mb_size, train_ratio)\r\n",
        "    test_x, test_y = get_test_data()\r\n",
        "\r\n",
        "    losses_mean_row = []\r\n",
        "    accs_mean_row   = []\r\n",
        "\r\n",
        "    for epoch in range(epoch_count):\r\n",
        "        losses = []\r\n",
        "        accs   = [] \r\n",
        "        for n in range(mini_batch_step_count):\r\n",
        "            train_x, train_y = get_train_data(mb_size, n)\r\n",
        "            loss, acc = run_train(train_x, train_y)\r\n",
        "            losses.append(loss)\r\n",
        "            accs.append(acc)\r\n",
        "\r\n",
        "        if report > 0 and (epoch + 1) % report == 0:\r\n",
        "            acc = run_test(test_x, test_y)\r\n",
        "            print(\"Epoch {} : Train - Loss = {:.3f}, Accuracy = {:.3f}  / Test - Accuracy = {:.3f}\".\\\r\n",
        "                  format(epoch + 1, np.mean(losses), np.mean(accs), acc))\r\n",
        "            \r\n",
        "        losses_mean = np.mean(losses)\r\n",
        "        accs_mean   = np.mean(accs) * 100\r\n",
        "\r\n",
        "        losses_mean_row.append(losses_mean)\r\n",
        "        accs_mean_row.append(accs_mean)\r\n",
        "\r\n",
        "    final_acc = run_test(test_x, test_y)        \r\n",
        "    print(\"=\" * 30, \"Final TEST\", \"=\" * 30)\r\n",
        "    print(\"\\nFinal Accuracy : {:.3f}\".format(final_acc))\r\n",
        "\r\n",
        "    return losses_mean_row, accs_mean_row, final_acc\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "Pc9lJeVarucl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "def forward_neuralnet(x):\r\n",
        "    y_hat = np.matmul(x, weight) + bias \r\n",
        "    return y_hat, x"
      ],
      "outputs": [],
      "metadata": {
        "id": "baask0ONopwA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "def forward_postproc(y_hat, y):\r\n",
        "    diff   = y_hat - y\r\n",
        "    square = np.square(diff)\r\n",
        "    loss   = np.mean(square)\r\n",
        "\r\n",
        "    return loss, diff"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZMFWYC_6op9Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "def eval_accuracy(y_hat, y):\r\n",
        "    mdiff = np.mean(np.abs((y_hat - y) / y))\r\n",
        "    return 1 - mdiff "
      ],
      "outputs": [],
      "metadata": {
        "id": "QM9XisDZqhLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "def backprop_neuralnet(G_output, x):\r\n",
        "    global weight, bias \r\n",
        "\r\n",
        "    x_transpose = x.transpose()\r\n",
        "    G_w = np.matmul(x_transpose, G_output)\r\n",
        "\r\n",
        "    G_b = np.sum(G_output, axis = 0)\r\n",
        "\r\n",
        "    weight -= LEARNING_RATE * G_w\r\n",
        "    bias   -= LEARNING_RATE * G_b"
      ],
      "outputs": [],
      "metadata": {
        "id": "0qyVoSGVvWa5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "def backprop_postproc(diff):\r\n",
        "    M_N = diff.shape\r\n",
        "\r\n",
        "    g_mse_square  = np.ones(M_N) / np.prod(M_N)\r\n",
        "    g_square_diff = 2 * diff\r\n",
        "    g_diff_output = 1 \r\n",
        "\r\n",
        "    G_diff   = g_mse_square * g_square_diff \r\n",
        "    G_output = g_diff_output * G_diff\r\n",
        "\r\n",
        "    return G_output"
      ],
      "outputs": [],
      "metadata": {
        "id": "3aB0jYUkvWgW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "def run_train(x, y):\r\n",
        "    y_hat, aux_nn_x   = forward_neuralnet(x)\r\n",
        "    loss, aux_pp_diff = forward_postproc(y_hat, y)\r\n",
        "\r\n",
        "    accuracy = eval_accuracy(y_hat, y)\r\n",
        "\r\n",
        "    G_output = backprop_postproc(aux_pp_diff)\r\n",
        "    backprop_neuralnet(G_output, aux_nn_x)\r\n",
        "\r\n",
        "    return loss, accuracy"
      ],
      "outputs": [],
      "metadata": {
        "id": "qLs80qpjvWiy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "def run_test(x, y):\r\n",
        "    y_hat, _  = forward_neuralnet(x)\r\n",
        "    accuracy = eval_accuracy(y_hat, y)\r\n",
        "\r\n",
        "    return accuracy"
      ],
      "outputs": [],
      "metadata": {
        "id": "x0lk_gjXqhRu"
      }
    }
  ]
}